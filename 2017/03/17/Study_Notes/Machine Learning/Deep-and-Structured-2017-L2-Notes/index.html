<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Deep and Structured(2017) - L2 Notes | NaiveRed&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Deep Learning for Language Modeling">
<meta name="keywords" content="Machine Learning,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep and Structured(2017) - L2 Notes">
<meta property="og:url" content="http://naivered.github.io/2017/03/17/Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes/index.html">
<meta property="og:site_name" content="NaiveRed&#39;s Blog">
<meta property="og:description" content="Deep Learning for Language Modeling">
<meta property="og:image" content="http://imgur.com/Q1q4Qwj.png">
<meta property="og:image" content="http://imgur.com/ByKAr4N.png">
<meta property="og:image" content="http://imgur.com/5LF1NvW.png">
<meta property="og:image" content="http://imgur.com/vR3gCh2.png">
<meta property="og:image" content="http://imgur.com/cWsDNow.png">
<meta property="og:image" content="http://imgur.com/ojLJ9B0.png">
<meta property="og:image" content="http://imgur.com/4zOtjOn.png">
<meta property="og:image" content="http://imgur.com/52GWzGu.png">
<meta property="og:image" content="http://imgur.com/xjJTo8X.png">
<meta property="og:image" content="http://imgur.com/M10otan.png">
<meta property="og:image" content="http://imgur.com/Ecnqsox.png">
<meta property="og:updated_time" content="2017-03-18T05:00:47.116Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep and Structured(2017) - L2 Notes">
<meta name="twitter:description" content="Deep Learning for Language Modeling">
<meta name="twitter:image" content="http://imgur.com/Q1q4Qwj.png">
  
    <link rel="alternate" href="/atom.xml" title="NaiveRed&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-73951673-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">NaiveRed&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Don&#39;t Panic!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://naivered.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/17/Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes/" class="article-date">
  <time datetime="2017-03-17T07:31:59.000Z" itemprop="datePublished">2017-03-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/學習筆記/">學習筆記</a>►<a class="article-category-link" href="/categories/學習筆記/ML-Hung-yi-Lee/">ML(Hung-yi Lee)</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Deep and Structured(2017) - L2 Notes
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
          
            <div id="toc" class="toc-article">
            <h2 class="toc-title">Contents</h2>
			  
                <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Deep-Learning-for-Language-Modeling"><span class="toc-number">1.</span> <span class="toc-text">Deep Learning for Language Modeling</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Language-model"><span class="toc-number">2.</span> <span class="toc-text">Language model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#N-gram"><span class="toc-number">2.1.</span> <span class="toc-text">N-gram</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NN-based-LM"><span class="toc-number">2.2.</span> <span class="toc-text">NN-based LM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN-based-LM"><span class="toc-number">2.3.</span> <span class="toc-text">RNN-based LM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更進一步"><span class="toc-number">2.4.</span> <span class="toc-text">更進一步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#N-gram-1"><span class="toc-number">2.4.1.</span> <span class="toc-text">N-gram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NN-based-LM-1"><span class="toc-number">2.4.2.</span> <span class="toc-text">NN-based LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-based-LM-1"><span class="toc-number">2.4.3.</span> <span class="toc-text">RNN-based LM</span></a></li></ol></li></ol></li></ol> 
              
            </div>
        
        <h1 id="Deep-Learning-for-Language-Modeling"><a href="#Deep-Learning-for-Language-Modeling" class="headerlink" title="Deep Learning for Language Modeling"></a>Deep Learning for Language Modeling</h1><a id="more"></a>
<h1 id="Language-model"><a href="#Language-model" class="headerlink" title="Language model"></a>Language model</h1><p>Language model : 用來估計一個句子所出現的機率。<br>而句子是由一連串的詞(word)所組成的。</p>
<p><img src="http://imgur.com/Q1q4Qwj.png" alt="2-2"></p>
<h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>那如何估算 $p$ 呢？</p>
<p>我們可以收集很多 data 然後直接計算 word sequence 所出現的機率 ，但很有可能這個句子根本沒出現在資料裡。</p>
<p>所以我們就把它拆成很多個詞來看，如圖中所示，如果只考慮前面一個 word 的話就是 <strong>2-gram</strong>(bigram) 。</p>
<p><img src="http://imgur.com/ByKAr4N.png" alt="2-3"></p>
<h2 id="NN-based-LM"><a href="#NN-based-LM" class="headerlink" title="NN-based LM"></a>NN-based LM</h2><p>訓練一個 network ，以第一個來看，輸入是 <em>潮水</em> 和 <em>退了</em> ，目標是 <em>就</em> 。然後我們去 minimize 輸出和<br>目標。</p>
<p><img src="http://imgur.com/5LF1NvW.png" alt="2-4"></p>
<p>以 wreck a nice beach 來看，其句子出現的機率就是: (wreck 在句首的機率) 乘 (wreck 後面接 a 的機率) 乘…  </p>
<p>而在 NN-based 中 我們並不是用統計來得到這些機率，而是 <strong>Network 的輸出</strong>。</p>
<p><img src="http://imgur.com/vR3gCh2.png" alt="2-5"></p>
<h2 id="RNN-based-LM"><a href="#RNN-based-LM" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h2><p>如同 NN-based 一樣，輸入 <em>begin</em> =&gt; 輸出 <em>潮水</em> , <em>潮水</em> =&gt; <em>退了</em> …  </p>
<p>不同的地方是 到了 <em>知道</em> 時，它已經看了 [<em>潮水、退了、就</em>] 才決定出 <em>知道</em> 這個詞。<br>在 NN-base 中則是只看現在的輸入來預測。</p>
<p><img src="http://imgur.com/cWsDNow.png" alt="2-6"></p>
<p>接著計算起來就像是把每次 network 的輸出相乘在一起。  </p>
<p><img src="http://imgur.com/ojLJ9B0.png" alt="2-7"></p>
<h2 id="更進一步"><a href="#更進一步" class="headerlink" title="更進一步"></a>更進一步</h2><h3 id="N-gram-1"><a href="#N-gram-1" class="headerlink" title="N-gram"></a>N-gram</h3><p>我們收集到的資料不夠多，無法正確統計。<br>以圖中例子來看，<em>dog</em> 也是有可能 jumped 的。  </p>
<p>可以利用 <strong>smoothing</strong> 的方法，也就是不直接讓它機率為 0 ，而是一個比較小的機率。</p>
<p><img src="http://imgur.com/4zOtjOn.png" alt="2-8"></p>
<p><strong>Matrix Factorization</strong> :</p>
<p>將 n-gram 的機率建成一個 table，每一個空格表示的是，給一個 history 下一個出現某 vocabulary 的機率。<br>$p(jumped|cat)$ = cat 後面接 jumped 的機率</p>
<p>然而可以看到有很多格 0 ，但那只是因為資料中沒有，不代表它不存在。</p>
<p>要怎麼解決這個問題呢？  </p>
<p>先將 vocabulary 和 history 變成向量 $v^i,h^j$ ，這個向量要從學習中產生。<br>而中間的元素則為 $n_{ij}$。<br>我們要去 minimizing $v,h$ 做內積後的值和 $n$ 越接近越好。(圖中的式子，也就是 loss function) </p>
<p>學完後，就能給每個詞彙一個 vector 了。<br>接著將 $n_{ij}$ 代換成 $v^i{\cdot}h^j$ ， 就能取代掉原本的 0 了。 </p>
<p><img src="http://imgur.com/52GWzGu.png" alt="2-9"></p>
<p>這樣做的好處在於如果兩個詞的向量相似，那他們對同個 $v$ 做內積的結果也會相似。<br>這跟一般 smoothing 差別在於，matrix factorization 有考慮到 這些詞彙的關係和意義。</p>
<p><img src="http://imgur.com/xjJTo8X.png" alt="2-10"></p>
<h3 id="NN-based-LM-1"><a href="#NN-based-LM-1" class="headerlink" title="NN-based LM"></a>NN-based LM</h3><p>Matrix Factorization 是可以寫成 NN 的，首先要做個轉換，因為每個 history 的 column 是對上後面接那個 vocabulary 的機率，所以我們要讓每個 column 的和為 1 。  </p>
<p>方法就是讓 $h,v$ 做完內積後，再丟去做 softmax ，接著在與 target(原本的 table ) 做比較(cross entropy)。</p>
<p><img src="http://imgur.com/M10otan.png" alt="2-11"></p>
<p>他們的 <strong>輸入</strong> 則是圖中橘色那塊，如果是 bigram ，輸入的維度就是 vocabulary size 。然後裡面的每一維與 hidden layer 相接的 weight 就是那一個 history 對應的 vector。</p>
<p>整個以 dog 為例，dog 那維是 1 ，其餘是 0 ，經過 hidden layer 後得到的就是 $h^dog$ 。<br>接著與 $v$ 做內積的部分又可以看成另一個 layer 再通過剛剛上面所講的過程。</p>
<p>還有一個好處是使用 NN-based 的話參數量遠小於 N-gram。(也就較不會 overfitting)<br>(嗚嗚開始寫黑板了…</p>
<h3 id="RNN-based-LM-1"><a href="#RNN-based-LM-1" class="headerlink" title="RNN-based LM"></a>RNN-based LM</h3><p>用 RNN 的話我們又可以減少更多參數。<br>如果我們的 history 非常的長，一連串的 $w$，透過同樣的 f 最後產生出的 $h^t$ 就拿來代表 history 。<br>再經由和每個 vocabulary 內積，通過 softmax ，就能得到下一個詞彙($w_{t+1}$)的機率。  </p>
<p>在訓練過程中 target 就是 $w_{t+1}$ 是哪一個詞，那一個就是 1 ，其餘是零(圖中右上)。</p>
<p><img src="http://imgur.com/Ecnqsox.png" alt="2-12"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://naivered.github.io/2017/03/17/Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes/" data-id="cjer0hrnb00mjl8ugbew8quhm" class="article-share-link">Share</a>
      
        <a href="http://naivered.github.io/2017/03/17/Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/24/Study_Notes/Machine Learning/ML17-Hello-world-Notes/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          ML17 - &quot;Hello World&quot; Notes
        
      </div>
    </a>
  
  
    <a href="/2017/03/16/Study_Notes/Machine Learning/Deep-and-Structured-2017-L1-Notes/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Deep and Structured(2017) - L1 Notes</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/14/Problem_Solving/UVa/UVa-11506-Angry-Programmer/">UVa 11506 - Angry Programmer</a>
          </li>
        
          <li>
            <a href="/2018/03/14/Problem_Solving/UVa/UVa-10530-Guessing-Game/">UVa 10530 - Guessing Game</a>
          </li>
        
          <li>
            <a href="/2018/03/13/Problem_Solving/UVa/UVa-440-Eeny-Meeny-Moo/">UVa 440 - Eeny Meeny Moo</a>
          </li>
        
          <li>
            <a href="/2018/03/12/Problem_Solving/UVa/UVa-924-Spreading-The-News/">UVa 924 - Spreading The News</a>
          </li>
        
          <li>
            <a href="/2018/03/11/Problem_Solving/UVa/UVa-10104-Euclid-Problem/">UVa 10104 - Euclid Problem</a>
          </li>
        
          <li>
            <a href="/2018/03/11/Problem_Solving/UVa/UVa-11420-Chest-of-Drawers/">UVa 11420 - Chest of Drawers</a>
          </li>
        
          <li>
            <a href="/2018/03/11/Problem_Solving/UVa/UVa-10819-Trouble-of-13-Dots/">UVa 10819 - Trouble of 13-Dots</a>
          </li>
        
          <li>
            <a href="/2018/03/11/Problem_Solving/UVa/UVa-796-Critical-Links/">UVa 796 - Critical Links</a>
          </li>
        
          <li>
            <a href="/2018/03/10/Problem_Solving/CodeForces/CodeForces-949A-Zebras/">CodeForces 949A - Zebras</a>
          </li>
        
          <li>
            <a href="/2018/03/10/Problem_Solving/UVa/UVa-10099-The-Tourist-Guide/">UVa 10099 - The Tourist Guide</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Problem-Solving/">Problem Solving</a><span class="category-list-count">232</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Problem-Solving/CodeForces/">CodeForces</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Problem-Solving/UVa/">UVa</a><span class="category-list-count">227</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Problem-Solving/ZeroJudge/">ZeroJudge</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/學習筆記/">學習筆記</a><span class="category-list-count">26</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/學習筆記/ML-Hung-yi-Lee/">ML(Hung-yi Lee)</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/學習筆記/Python/">Python</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/學習筆記/機器學習基石/">機器學習基石</a><span class="category-list-count">11</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/心情隨筆/">心情隨筆</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/網頁建置/">網頁建置</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/網頁建置/Hexo/">Hexo</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/讀書手札/">讀書手札</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/讀書手札/讀後感/">讀後感</a><span class="category-list-count">1</span></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">23</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">48</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">52</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">18</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">29</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 NaiveRed<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and Theme by <a href="https://github.com/hexojs/hexo-theme-landscape" target="_blank">landscape</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
</nav>
    
<script>
  var disqus_shortname = 'naivered';
  
  var disqus_url = 'http://naivered.github.io/2017/03/17/Study_Notes/Machine Learning/Deep-and-Structured-2017-L2-Notes/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>

<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
</body>
</html>